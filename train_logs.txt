TRL version: 0.23.1
PEFT version: 0.17.1
TensorBoard version: 2.20.0
Loading processor and dataset...
==============================================================
Loading model...
==============================================================
Setting up training configuration...
==============================================================
Initializing DPO trainer...
==============================================================
Starting training...
{'loss': 0.6934, 'grad_norm': 3.3249874114990234, 'learning_rate': 9.274193548387096e-07, 'rewards/chosen': 0.029392287135124207, 'rewards/rejected': 0.028889194130897522, 'rewards/accuracies': 0.46406251192092896, 'rewards/margins': 0.0005030919564887881, 'logps/chosen': -716.6146240234375, 'logps/rejected': -757.5062866210938, 'logits/chosen': -2.102160692214966, 'logits/rejected': -2.098186731338501, 'epoch': 0.08, 'num_input_tokens_seen': 0, 'train_runtime': 256.2288, 'train_tokens_per_second': 0.0}
{'loss': 0.6943, 'grad_norm': 1.7935547828674316, 'learning_rate': 8.467741935483871e-07, 'rewards/chosen': 0.032427944242954254, 'rewards/rejected': 0.033686768263578415, 'rewards/accuracies': 0.48750001192092896, 'rewards/margins': -0.0012588215759024024, 'logps/chosen': -707.7560424804688, 'logps/rejected': -745.7142333984375, 'logits/chosen': -2.1102843284606934, 'logits/rejected': -2.0884506702423096, 'epoch': 0.16, 'num_input_tokens_seen': 0, 'train_runtime': 510.2828, 'train_tokens_per_second': 0.0}
{'loss': 0.695, 'grad_norm': 2.919945478439331, 'learning_rate': 7.661290322580645e-07, 'rewards/chosen': 0.020257551223039627, 'rewards/rejected': 0.022839628159999847, 'rewards/accuracies': 0.4765625, 'rewards/margins': -0.0025820767041295767, 'logps/chosen': -686.73193359375, 'logps/rejected': -730.2444458007812, 'logits/chosen': -2.1019997596740723, 'logits/rejected': -2.0829367637634277, 'epoch': 0.24, 'num_input_tokens_seen': 0, 'train_runtime': 762.7613, 'train_tokens_per_second': 0.0}
{'loss': 0.6925, 'grad_norm': 2.634859085083008, 'learning_rate': 6.854838709677419e-07, 'rewards/chosen': 0.019724395126104355, 'rewards/rejected': 0.017230873927474022, 'rewards/accuracies': 0.5078125, 'rewards/margins': 0.0024935200344771147, 'logps/chosen': -696.5640869140625, 'logps/rejected': -725.544677734375, 'logits/chosen': -2.099208116531372, 'logits/rejected': -2.0768465995788574, 'epoch': 0.32, 'num_input_tokens_seen': 0, 'train_runtime': 1014.8863, 'train_tokens_per_second': 0.0}
{'loss': 0.6918, 'grad_norm': 0.8675636053085327, 'learning_rate': 6.048387096774193e-07, 'rewards/chosen': 0.016642572358250618, 'rewards/rejected': 0.012511116452515125, 'rewards/accuracies': 0.520312488079071, 'rewards/margins': 0.004131455905735493, 'logps/chosen': -694.7318115234375, 'logps/rejected': -753.2335815429688, 'logits/chosen': -2.104447603225708, 'logits/rejected': -2.0931496620178223, 'epoch': 0.41, 'num_input_tokens_seen': 0, 'train_runtime': 1268.2883, 'train_tokens_per_second': 0.0}
{'loss': 0.6925, 'grad_norm': 2.802246570587158, 'learning_rate': 5.241935483870967e-07, 'rewards/chosen': 0.004435100592672825, 'rewards/rejected': 0.0017310477560386062, 'rewards/accuracies': 0.518750011920929, 'rewards/margins': 0.002704052720218897, 'logps/chosen': -713.14990234375, 'logps/rejected': -748.9664306640625, 'logits/chosen': -2.1097309589385986, 'logits/rejected': -2.0878608226776123, 'epoch': 0.49, 'num_input_tokens_seen': 0, 'train_runtime': 1526.2181, 'train_tokens_per_second': 0.0}
{'loss': 0.692, 'grad_norm': 1.5550206899642944, 'learning_rate': 4.4354838709677415e-07, 'rewards/chosen': -0.024331428110599518, 'rewards/rejected': -0.02800178900361061, 'rewards/accuracies': 0.503125011920929, 'rewards/margins': 0.0036703585647046566, 'logps/chosen': -685.6779174804688, 'logps/rejected': -731.1978759765625, 'logits/chosen': -2.0934269428253174, 'logits/rejected': -2.0791172981262207, 'epoch': 0.57, 'num_input_tokens_seen': 0, 'train_runtime': 1778.8129, 'train_tokens_per_second': 0.0}
{'loss': 0.6904, 'grad_norm': 1.2738498449325562, 'learning_rate': 3.629032258064516e-07, 'rewards/chosen': -0.037974681705236435, 'rewards/rejected': -0.0451231449842453, 'rewards/accuracies': 0.5453125238418579, 'rewards/margins': 0.007148459553718567, 'logps/chosen': -681.788818359375, 'logps/rejected': -730.1643676757812, 'logits/chosen': -2.089430093765259, 'logits/rejected': -2.0787365436553955, 'epoch': 0.65, 'num_input_tokens_seen': 0, 'train_runtime': 2030.3471, 'train_tokens_per_second': 0.0}
{'loss': 0.6902, 'grad_norm': 1.102342128753662, 'learning_rate': 2.8225806451612905e-07, 'rewards/chosen': -0.046798352152109146, 'rewards/rejected': -0.05444451421499252, 'rewards/accuracies': 0.526562511920929, 'rewards/margins': 0.007646174170076847, 'logps/chosen': -702.8707885742188, 'logps/rejected': -740.6410522460938, 'logits/chosen': -2.0814831256866455, 'logits/rejected': -2.0809154510498047, 'epoch': 0.73, 'num_input_tokens_seen': 0, 'train_runtime': 2282.9341, 'train_tokens_per_second': 0.0}
{'loss': 0.6883, 'grad_norm': 3.9946298599243164, 'learning_rate': 2.0161290322580642e-07, 'rewards/chosen': -0.04528668150305748, 'rewards/rejected': -0.05667326971888542, 'rewards/accuracies': 0.5531250238418579, 'rewards/margins': 0.011386587284505367, 'logps/chosen': -695.0792846679688, 'logps/rejected': -730.0106201171875, 'logits/chosen': -2.102102518081665, 'logits/rejected': -2.0864272117614746, 'epoch': 0.81, 'num_input_tokens_seen': 0, 'train_runtime': 2534.8272, 'train_tokens_per_second': 0.0}
{'loss': 0.6897, 'grad_norm': 2.5204286575317383, 'learning_rate': 1.2096774193548387e-07, 'rewards/chosen': -0.04364946857094765, 'rewards/rejected': -0.05244380980730057, 'rewards/accuracies': 0.543749988079071, 'rewards/margins': 0.008794348686933517, 'logps/chosen': -691.2691650390625, 'logps/rejected': -727.546142578125, 'logits/chosen': -2.1051979064941406, 'logits/rejected': -2.0925517082214355, 'epoch': 0.89, 'num_input_tokens_seen': 0, 'train_runtime': 2786.1444, 'train_tokens_per_second': 0.0}
{'loss': 0.6887, 'grad_norm': 0.7460408806800842, 'learning_rate': 4.032258064516129e-08, 'rewards/chosen': -0.048208195716142654, 'rewards/rejected': -0.05889473110437393, 'rewards/accuracies': 0.5390625, 'rewards/margins': 0.010686539113521576, 'logps/chosen': -701.43701171875, 'logps/rejected': -730.0784912109375, 'logits/chosen': -2.0900862216949463, 'logits/rejected': -2.079742431640625, 'epoch': 0.97, 'num_input_tokens_seen': 0, 'train_runtime': 3039.3267, 'train_tokens_per_second': 0.0}
{'train_runtime': 3118.6671, 'train_samples_per_second': 2.526, 'train_steps_per_second': 0.04, 'train_loss': 0.6913161181634472, 'epoch': 1.0, 'num_input_tokens_seen': 0}
==============================================================
Training completed! Model saved at: ./llava-dpo-1.5
